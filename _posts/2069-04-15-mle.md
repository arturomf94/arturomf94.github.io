## Kullback-Leibler Divergence, Entropy, and Maximum Likelihood Estimators: A Concise Overview

This post is focused on the the relationship between Kullback-Leibler (KL) divergence, entropy, and Maximum Likelihood Estimators (MLE). These concepts are important tools in information theory and statistical inference, and understanding their connections can provide valuable insights.

### Kullback-Leibler Divergence

Kullback-Leibler divergence, also known as relative entropy, is a measure of the difference between two probability distributions. Given two discrete probability distributions $P$ and $Q$, the KL divergence is defined as:

$$
D_{KL}(P||Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
$$

The KL divergence is always non-negative, and it is equal to zero if and only if $P$ and $Q$ are the same distribution.

### Entropy

Entropy, denoted by $H(P)$, is a measure of the uncertainty or randomness of a probability distribution $P$. For a discrete probability distribution, entropy is defined as:

$$
H(P) = -\sum_{i} P(x_i) \log P(x_i)
$$

The entropy is maximized when the distribution is uniform, which means all outcomes are equally likely.

### Maximum Likelihood Estimators

Maximum Likelihood Estimation is a statistical method for estimating the parameters of a model. Given a set of observed data $X$ and a parametric model with a probability distribution $P_\theta$, the MLE aims to find the parameter value $\theta$ that maximizes the likelihood function:

$$
\hat{\theta}_{MLE} = \arg\max_\theta \prod_{i=1}^{n} P_\theta(x_i)
$$

Equivalently, we can maximize the log-likelihood:

$$
\hat{\theta}_{MLE} = \arg\max_\theta \sum_{i=1}^{n} \log P_\theta(x_i)
$$

### Relationship between KL Divergence, Entropy, and MLE

Now let's explore the relationship between KL divergence, entropy, and MLE. Suppose we have a true unknown distribution $P$ and a parametric model $Q_\theta$. The goal is to find the best $\theta$ that approximates $P$.

We can think of minimizing the KL divergence between $P$ and $Q_\theta$:

$$
\hat{\theta}_{KL} = \arg\min_\theta D_{KL}(P||Q_\theta) = \arg\min_\theta \sum_i P(x_i) \log \frac{P(x_i)}{Q_\theta(x_i)}
$$

Since $P(x_i)$ is independent of $\theta$, minimizing KL divergence is equivalent to maximizing the cross-entropy:

$$
\hat{\theta}_{KL} = \arg\max_\theta -\sum_i P(x_i) \log Q_\theta(x_i)
$$

In practice, we don't know the true distribution $P$, but we can approximate it using the empirical distribution $\hat{P}$ based on our observed data. This leads us to the following maximization problem:

$$
\hat{\theta}_{KL} \approx \arg\max_\theta -\sum_i \hat{P}(x_i) \log Q_\theta(x_i) = \arg\max_\theta \sum_{i=1}^{n} \log Q_\theta(x_i)
$$

This is the same objective function as in the MLE, which reveals the connection between minimizing KL divergence and maximizing the likelihood:

$$
\hat{\theta}_{MLE} \approx \hat{\theta}_{KL}
$$
In conclusion, minimizing the Kullback-Leibler divergence between an unknown true distribution $P$ and a parametric model $Q_\theta$ is closely related to maximizing the likelihood for the model parameters.
